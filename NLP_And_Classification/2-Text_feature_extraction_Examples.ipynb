{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xpIPMNPoU_-p"
   },
   "source": [
    "# Examples: Text feature extraction\n",
    "© ExploreAI Academy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will delve into text feature extraction techniques, focusing on the bag-of-words model and n-grams. We'll explore how to transform text data into feature sets usable by classifiers, particularly using the NLTK library. The bag-of-words model simplifies text into word presence features, while n-grams capture combinations of words to extract deeper meaning from text. \n",
    "\n",
    "Text feature extraction is the process of converting raw text data into numerical features that can be used for machine learning algorithms. In natural language processing (NLP), text feature extraction is a crucial step because most machine learning algorithms require numerical input.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you should be able to:\n",
    "* Understand the bag-of-words model and its role in text feature extraction.\n",
    "* Implement the bag-of-words model to transform text data into feature sets.\n",
    "* Explain the concept of n-grams and their significance in capturing combinations of words.\n",
    "* Use n-grams to extract contextual information from text data.\n",
    "* Fine-tune CountVectorizer parameters for optimal text feature extraction.\n",
    "\n",
    "\n",
    "Before we get started, let's get the data and the  libraries we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "# Set the path to the CA certificates bundle\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQm0O5XHU_-z",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "import string\n",
    "\n",
    "# set plot style\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29356,
     "status": "error",
     "timestamp": 1560340175121,
     "user": {
      "displayName": "Bryan Davies",
      "photoUrl": "",
      "userId": "03059035420523728518"
     },
     "user_tz": -120
    },
    "id": "w8Iw1yCRU_-2",
    "outputId": "188501d1-fcf6-45be-8a45-56f885491dcd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/chris/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/chris/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download()\n",
    "# or you can download directly, i.e.\n",
    "nltk.download(['punkt','stopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with our `MBTI` dataset, let's read the data and clean it up a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the MBTI dataset\n",
    "mbti = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/classification_sprint/mbti_train.csv')\n",
    "mbti.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>http://41.media.tumblr.com/tumblr_lfouy03PMA1q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp and intj moments  https://www.youtube.com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>What has been the most life-changing experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>http://www.youtube.com/watch?v=vXZeYwwRDw8   h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316543</th>\n",
       "      <td>INFP</td>\n",
       "      <td>Kallinhausin, you may have just rooted out the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316544</th>\n",
       "      <td>INFP</td>\n",
       "      <td>In regards to the king, (in the show, not in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316545</th>\n",
       "      <td>INFP</td>\n",
       "      <td>Sunlight bouncing off the fog at dawn.  Serend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316546</th>\n",
       "      <td>INFP</td>\n",
       "      <td>Songs are really powerful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316547</th>\n",
       "      <td>INFP</td>\n",
       "      <td>I just have to remember they weren't trying to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316548 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        type                                               post\n",
       "0       INFJ        'http://www.youtube.com/watch?v=qsXHcwe3krw\n",
       "1       INFJ  http://41.media.tumblr.com/tumblr_lfouy03PMA1q...\n",
       "2       INFJ  enfp and intj moments  https://www.youtube.com...\n",
       "3       INFJ  What has been the most life-changing experienc...\n",
       "4       INFJ  http://www.youtube.com/watch?v=vXZeYwwRDw8   h...\n",
       "...      ...                                                ...\n",
       "316543  INFP  Kallinhausin, you may have just rooted out the...\n",
       "316544  INFP  In regards to the king, (in the show, not in t...\n",
       "316545  INFP  Sunlight bouncing off the fog at dawn.  Serend...\n",
       "316546  INFP                         Songs are really powerful.\n",
       "316547  INFP  I just have to remember they weren't trying to...\n",
       "\n",
       "[316548 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate each post in the 'posts' column into its own row\n",
    "all_mbti = []\n",
    "for i, row in mbti.iterrows():\n",
    "    for post in row['posts'].split('|||'):\n",
    "        all_mbti.append([row['type'], post])\n",
    "all_mbti = pd.DataFrame(all_mbti, columns=['type', 'post'])\n",
    "\n",
    "all_mbti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove noise\n",
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "subs_url = r'url-web'\n",
    "all_mbti['post'] = all_mbti['post'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "all_mbti['post'] = all_mbti['post'].str.lower()\n",
    "\n",
    "#Remove puntuation\n",
    "def remove_punctuation(post):\n",
    "    return ''.join([l for l in post if l not in string.punctuation])\n",
    "\n",
    "all_mbti['post'] = all_mbti['post'].apply(remove_punctuation)\n",
    "\n",
    "# Tokenize the text using the TreebankWordTokenizer\n",
    "tokeniser = TreebankWordTokenizer()\n",
    "all_mbti['tokens'] = all_mbti['post'].apply(tokeniser.tokenize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text feature extraction\n",
    "\n",
    "### Bag of words\n",
    "\n",
    "The Bag of Words (BoW) model is a simple and commonly used technique for text feature extraction in natural language processing (NLP). It represents text data as a collection of words, ignoring grammar and word order. The BoW model focuses solely on the presence or absence of words in a document and their frequency of occurrence.\n",
    "\n",
    "Here's how the Bag of Words model works:\n",
    "\n",
    "- Vocabulary Creation: First, the model builds a vocabulary, which is a list of unique words (or tokens) that appear in the entire corpus of documents. Each word in the vocabulary is assigned a unique index.\n",
    "\n",
    "- Vectorization: Next, each document in the corpus is represented as a vector, where the dimensions of the vector correspond to the words in the vocabulary. The value in each dimension represents the frequency of the corresponding word in the document. If a word appears multiple times in a document, its frequency count is incremented accordingly.\n",
    "\n",
    "- Sparse Representation: Since most documents contain only a small subset of the words in the vocabulary, the resulting vectors are typically sparse (i.e., mostly zeros). This sparse representation is memory-efficient and suitable for high-dimensional data.\n",
    "\n",
    "- Normalization: Optionally, the frequency counts in the vectors can be normalized to account for differences in document length or the overall frequency of words in the corpus. Common normalization techniques include TF-IDF (Term Frequency-Inverse Document Frequency).\n",
    "\n",
    "\n",
    "Here's a basic example of implementing the Bag of Words model in Python using the CountVectorizer class from scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "\n",
      "BoW representation:\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the documents and transform the documents into BoW vectors\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the sparse matrix to a dense array for visualization (not recommended for large datasets)\n",
    "bow_array = bow_matrix.toarray()\n",
    "\n",
    "# Display the vocabulary and BoW representation of the documents\n",
    "print(\"Vocabulary:\")\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"\\nBoW representation:\")\n",
    "print(bow_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BbRb9FHYVAAg"
   },
   "source": [
    "Text feature extraction is the process of transforming what is essentially a list of words into a feature set that is usable by a classifier. The NLTK classifiers expect `dict` style feature sets, so we must therefore transform our text into a Python dictionary object. The Bag of Words model is the simplest method; it constructs a word presence feature set from all the words in the text, indicating the number of times each word has appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mg8PtprJVAAg",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def bag_of_words_count(words, word_dict={}):\n",
    "    \"\"\" this function takes in a list of words and returns a dictionary \n",
    "        with each word as a key, and the value represents the number of \n",
    "        times that word appeared\"\"\"\n",
    "    for word in words:\n",
    "        if word in word_dict.keys():\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a set of dictionaries, one for each of the MBTI types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of all the MBTI personality types that are present in the original dataset\n",
    "type_labels = list(all_mbti.type.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TtqJO_YhVAAh",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "personality = {}\n",
    "for pp in type_labels:\n",
    "    df = all_mbti.groupby('type')\n",
    "    personality[pp] = {}\n",
    "    for row in df.get_group(pp)['tokens']:\n",
    "        personality[pp] = bag_of_words_count(row, personality[pp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a list of all of the unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ky-rofa_VAAi",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_words = set()\n",
    "for pp in type_labels:\n",
    "    for word in personality[pp]:\n",
    "        all_words.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was done so that we can create a combined bag of words dictionary for all the words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bpjTsIOsVAAl",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "personality['all'] = {}\n",
    "for pp in type_labels:    \n",
    "    for word in all_words:\n",
    "        if word in personality[pp].keys():\n",
    "            if word in personality['all']:\n",
    "                personality['all'][word] += personality[pp][word]\n",
    "            else:\n",
    "                personality['all'][word] = personality[pp][word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily calculate how many words there are in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BUGBaAELVAAp",
    "outputId": "beff7596-19e4-43bb-ecce-291231be4c21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8203466"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words = sum([v for v in personality['all'].values()])\n",
    "total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of words which occur less than 10 times in the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aiSrw4V3VAAn",
    "outputId": "b59469fe-8252-4546-94b3-76c1d9ac59bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'word frequency')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist([v for v in personality['all'].values() if v < 10],bins=10)\n",
    "plt.ylabel(\"# of words\")\n",
    "plt.xlabel(\"word frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QspSnpFzVAAp"
   },
   "source": [
    "There are a lot of words that only appear once! We'll print out that value here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lBsFSL29VAAw",
    "outputId": "64e625a1-15be-4541-b802-632e803b903a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81268"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([v for v in personality['all'].values() if v == 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of words do you think would appear once? Let's print out a few of these rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ava82', 'humanrenaisance', 'amsklfmlcv', 'geoscience', 'dayim', 'dreamhowever', 'wowsuch', '837e6e', 'bluesecondary', 'fourletterword', 'eglinton', 'ignominy', 'disorientatedly', 'urlwebstart', '1724', 'postmodernist', 'littleknown', 'deceving', 'drops”', 'squids88379', 'kidif', 'personer', 'patriarch', 'jakarta', 'x93unidentified', 'thoughtwhy', 'blahthat', 'ncv', 'anledningen', '1z0100', 'longblushed', '13i', 'damfino', 'kyfra', 'elsemaybe', 'inattentiveness', '2cups', 'dreamiest', 'unknownand', 'responsesi', 'universex94', '282786', '2286', 'mertz', 'urlwebpost872302', 'loserlike', 'morrisette', 'solipsists', 'quasimorphine', 'charactersalphabets', 'implicitlyexplicitly', 'jonne', 'hummi', 'unsent', 'sovari', 'luckthing', '107hz', 'versace', 'angryshhhiiiiieeeeeeeeeeeeeet', 'smileyssunbathingsmileyssunbathing', 'hwccxhr44ampindex147', 'corroborated', 'wantslets', 'dok', 'x97wow', 'voncloft', 'herepeople', 'constantit', 'scremoish', 'contributeadd', '100yen', 'merola', 'nowhappy', '716745', 'epressing', 'smilinglaughing', 'personin', 'feareth', 'minnesotain', 'valkyria', 'deydey', 'brotherssad', 'leidseplein', 'alwayscheaters', 'humourthe', 'willingy', 'muchbutid', 'sonja', 'x202ajuanes', 'insulationsound', 'notsofunny', '431', 'pogeyman', 'talkmoan', 'smileblushlaughsideeye', 'bosscool', 'ccr', 'sucksim', 'dunkirk★★★☆☆', 'threadconfused']\n"
     ]
    }
   ],
   "source": [
    "rare_words = [k for k, v in personality['all'].items() if v==1] \n",
    "print(rare_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, some of these words don't make sense, but before we decide to remove them, let's see how much data we'll be left with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "amu9ABO8VAAz",
    "outputId": "3d99bd9f-f5eb-4c26-f3e4-d1751eee8920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18193\n",
      "7998579\n"
     ]
    }
   ],
   "source": [
    "# how many words appear more than 10 times?\n",
    "# how many words of the total does that account for?\n",
    "print(len([v for v in personality['all'].values() if v >= 10]))\n",
    "occurs_more_than_10_times = sum([v for v in personality['all'].values() if v >= 10])\n",
    "print(occurs_more_than_10_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwmyzJewVAA2",
    "outputId": "7e64933d-1034-4c8f-d160-36472eeb154f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9750243372740254"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occurs_more_than_10_times/total_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TxW-TeheVAA4"
   },
   "source": [
    "Using words that appear more than 10 times seems much more useful!  And this accounts for 97% of all the words!\n",
    "\n",
    "Finally, let's remove all words that occur less than 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZNdn2n6VAA4",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "max_count = 10\n",
    "remaining_word_index = [k for k, v in personality['all'].items() if v > max_count]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis testing\n",
    "Remember our Hypothesis from earlier?:\n",
    "\n",
    "- Introverts tend to use the word `I` more than extroverts\n",
    "- Conversely, Extroverts tend to favour the word `you`\n",
    "\n",
    "Let's see if we finally have what we need to test it out. We'll first create one big dataframe with the word counts by personality profile (this may take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fW5qiwvrVAA5",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "hm = []\n",
    "for p, p_bow in personality.items():\n",
    "    df_bow = pd.DataFrame([(k, v) for k, v in p_bow.items() if k in remaining_word_index], columns=['Word', p])\n",
    "    df_bow.set_index('Word', inplace=True)\n",
    "    hm.append(df_bow)\n",
    "\n",
    "# create one big dataframe\n",
    "df_bow = pd.concat(hm, axis=1)\n",
    "df_bow.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top 10 words which appear most often?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaKScx05VAA7",
    "outputId": "5bcaa326-858b-478f-9f88-f582ce599a20"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INFJ</th>\n",
       "      <th>ENTP</th>\n",
       "      <th>INTP</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>ENTJ</th>\n",
       "      <th>ENFJ</th>\n",
       "      <th>INFP</th>\n",
       "      <th>ENFP</th>\n",
       "      <th>ISFP</th>\n",
       "      <th>ISTP</th>\n",
       "      <th>ISFJ</th>\n",
       "      <th>ISTJ</th>\n",
       "      <th>ESTP</th>\n",
       "      <th>ESFP</th>\n",
       "      <th>ESTJ</th>\n",
       "      <th>ESFJ</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>67821.0</td>\n",
       "      <td>27381.0</td>\n",
       "      <td>52046.0</td>\n",
       "      <td>43810.0</td>\n",
       "      <td>8875.0</td>\n",
       "      <td>8683.0</td>\n",
       "      <td>87642.0</td>\n",
       "      <td>31156.0</td>\n",
       "      <td>11148.0</td>\n",
       "      <td>13883.0</td>\n",
       "      <td>8037.0</td>\n",
       "      <td>8169.0</td>\n",
       "      <td>3704.0</td>\n",
       "      <td>1696.0</td>\n",
       "      <td>1856.0</td>\n",
       "      <td>2166.0</td>\n",
       "      <td>378073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>39658.0</td>\n",
       "      <td>18993.0</td>\n",
       "      <td>35864.0</td>\n",
       "      <td>30497.0</td>\n",
       "      <td>6132.0</td>\n",
       "      <td>5018.0</td>\n",
       "      <td>48004.0</td>\n",
       "      <td>16454.0</td>\n",
       "      <td>6131.0</td>\n",
       "      <td>8893.0</td>\n",
       "      <td>4111.0</td>\n",
       "      <td>5141.0</td>\n",
       "      <td>2191.0</td>\n",
       "      <td>937.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>230224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>40231.0</td>\n",
       "      <td>17852.0</td>\n",
       "      <td>33005.0</td>\n",
       "      <td>28753.0</td>\n",
       "      <td>5889.0</td>\n",
       "      <td>5471.0</td>\n",
       "      <td>48996.0</td>\n",
       "      <td>16945.0</td>\n",
       "      <td>6264.0</td>\n",
       "      <td>8725.0</td>\n",
       "      <td>4607.0</td>\n",
       "      <td>5106.0</td>\n",
       "      <td>2254.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>1223.0</td>\n",
       "      <td>227371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>31931.0</td>\n",
       "      <td>14728.0</td>\n",
       "      <td>26692.0</td>\n",
       "      <td>22778.0</td>\n",
       "      <td>4748.0</td>\n",
       "      <td>3966.0</td>\n",
       "      <td>40375.0</td>\n",
       "      <td>13846.0</td>\n",
       "      <td>4825.0</td>\n",
       "      <td>7124.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>4033.0</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>796.0</td>\n",
       "      <td>841.0</td>\n",
       "      <td>986.0</td>\n",
       "      <td>182870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>31628.0</td>\n",
       "      <td>14236.0</td>\n",
       "      <td>24880.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>4564.0</td>\n",
       "      <td>4343.0</td>\n",
       "      <td>40709.0</td>\n",
       "      <td>15002.0</td>\n",
       "      <td>5153.0</td>\n",
       "      <td>6540.0</td>\n",
       "      <td>3571.0</td>\n",
       "      <td>3827.0</td>\n",
       "      <td>1905.0</td>\n",
       "      <td>834.0</td>\n",
       "      <td>943.0</td>\n",
       "      <td>988.0</td>\n",
       "      <td>180691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>24312.0</td>\n",
       "      <td>11335.0</td>\n",
       "      <td>21372.0</td>\n",
       "      <td>17857.0</td>\n",
       "      <td>3499.0</td>\n",
       "      <td>3114.0</td>\n",
       "      <td>29576.0</td>\n",
       "      <td>10217.0</td>\n",
       "      <td>3580.0</td>\n",
       "      <td>4962.0</td>\n",
       "      <td>2475.0</td>\n",
       "      <td>2976.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>650.0</td>\n",
       "      <td>779.0</td>\n",
       "      <td>138561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>22207.0</td>\n",
       "      <td>10870.0</td>\n",
       "      <td>17186.0</td>\n",
       "      <td>15997.0</td>\n",
       "      <td>3815.0</td>\n",
       "      <td>3048.0</td>\n",
       "      <td>24954.0</td>\n",
       "      <td>10315.0</td>\n",
       "      <td>3331.0</td>\n",
       "      <td>4696.0</td>\n",
       "      <td>2185.0</td>\n",
       "      <td>2731.0</td>\n",
       "      <td>1396.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>124672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>19444.0</td>\n",
       "      <td>8947.0</td>\n",
       "      <td>16384.0</td>\n",
       "      <td>14432.0</td>\n",
       "      <td>2906.0</td>\n",
       "      <td>2611.0</td>\n",
       "      <td>23437.0</td>\n",
       "      <td>8583.0</td>\n",
       "      <td>2931.0</td>\n",
       "      <td>4054.0</td>\n",
       "      <td>2033.0</td>\n",
       "      <td>2207.0</td>\n",
       "      <td>1063.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>521.0</td>\n",
       "      <td>615.0</td>\n",
       "      <td>110718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>18357.0</td>\n",
       "      <td>8257.0</td>\n",
       "      <td>15685.0</td>\n",
       "      <td>13167.0</td>\n",
       "      <td>2601.0</td>\n",
       "      <td>2278.0</td>\n",
       "      <td>22511.0</td>\n",
       "      <td>8010.0</td>\n",
       "      <td>2909.0</td>\n",
       "      <td>4244.0</td>\n",
       "      <td>2046.0</td>\n",
       "      <td>2285.0</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>469.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>104808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>18237.0</td>\n",
       "      <td>8903.0</td>\n",
       "      <td>15888.0</td>\n",
       "      <td>14293.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>2404.0</td>\n",
       "      <td>21068.0</td>\n",
       "      <td>7769.0</td>\n",
       "      <td>2726.0</td>\n",
       "      <td>3704.0</td>\n",
       "      <td>1879.0</td>\n",
       "      <td>2186.0</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>554.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>104779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         INFJ     ENTP     INTP     INTJ    ENTJ    ENFJ     INFP     ENFP  \\\n",
       "Word                                                                         \n",
       "i     67821.0  27381.0  52046.0  43810.0  8875.0  8683.0  87642.0  31156.0   \n",
       "the   39658.0  18993.0  35864.0  30497.0  6132.0  5018.0  48004.0  16454.0   \n",
       "to    40231.0  17852.0  33005.0  28753.0  5889.0  5471.0  48996.0  16945.0   \n",
       "a     31931.0  14728.0  26692.0  22778.0  4748.0  3966.0  40375.0  13846.0   \n",
       "and   31628.0  14236.0  24880.0  21568.0  4564.0  4343.0  40709.0  15002.0   \n",
       "of    24312.0  11335.0  21372.0  17857.0  3499.0  3114.0  29576.0  10217.0   \n",
       "you   22207.0  10870.0  17186.0  15997.0  3815.0  3048.0  24954.0  10315.0   \n",
       "that  19444.0   8947.0  16384.0  14432.0  2906.0  2611.0  23437.0   8583.0   \n",
       "it    18357.0   8257.0  15685.0  13167.0  2601.0  2278.0  22511.0   8010.0   \n",
       "is    18237.0   8903.0  15888.0  14293.0  3000.0  2404.0  21068.0   7769.0   \n",
       "\n",
       "         ISFP     ISTP    ISFJ    ISTJ    ESTP    ESFP    ESTJ    ESFJ     all  \n",
       "Word                                                                            \n",
       "i     11148.0  13883.0  8037.0  8169.0  3704.0  1696.0  1856.0  2166.0  378073  \n",
       "the    6131.0   8893.0  4111.0  5141.0  2191.0   937.0  1000.0  1200.0  230224  \n",
       "to     6264.0   8725.0  4607.0  5106.0  2254.0   972.0  1078.0  1223.0  227371  \n",
       "a      4825.0   7124.0  3333.0  4033.0  1868.0   796.0   841.0   986.0  182870  \n",
       "and    5153.0   6540.0  3571.0  3827.0  1905.0   834.0   943.0   988.0  180691  \n",
       "of     3580.0   4962.0  2475.0  2976.0  1300.0   557.0   650.0   779.0  138561  \n",
       "you    3331.0   4696.0  2185.0  2731.0  1396.0   651.0   651.0   639.0  124672  \n",
       "that   2931.0   4054.0  2033.0  2207.0  1063.0   550.0   521.0   615.0  110718  \n",
       "it     2909.0   4244.0  2046.0  2285.0  1065.0   434.0   469.0   490.0  104808  \n",
       "is     2726.0   3704.0  1879.0  2186.0  1121.0   482.0   554.0   565.0  104779  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow.sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5MTEdKrZVAA8"
   },
   "source": [
    "This isn't very helpful at all, is it? It's very difficult to extract insights from this data.  Let's see if we can use the $chi^2$ test to see whether Introverts favour the word **`I`**. \n",
    "\n",
    "The $chi^2$ test looks at observed versus expected results and lets us know where the greatest differences from expected values are.  The bigger the statistic, the greater the difference from expectation.  The formula is \n",
    "\n",
    "$$𝑐ℎ𝑖^2 = \\sum{\\frac{(𝑂𝑏𝑠𝑒𝑟𝑣𝑒𝑑 −𝑒𝑥𝑝𝑒𝑐𝑡𝑒𝑑)^2}{𝑒𝑥𝑝𝑒𝑐𝑡𝑒𝑑}}$$\n",
    "\n",
    "The $chi^2$ test will compare the **observed frequencies** of word usage by **introverts** to the **expected frequencies** based on the overall population and indicate the extent of this difference for each word.\n",
    "\n",
    "Using the $chi^2$ statistic over simply comparing the observed percentages, i.e `I_perc`, means that we are considering both the observed (or word usage by introverts) and expected frequencies(or the overall populations word usage) for each word, taking into account the sample size. This helps us determine whether the differences between observed and expected frequencies are statistically significant, accounting for variability due to sample size.\n",
    "\n",
    "We'll do this first by extracting introvert types only from all the personality types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2eTstsUzVAA8",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "intro_types_i = [p for p in type_labels if p[0] == 'I']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create an introvert total word count column, which sums the counts of all introvert columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7F3a0V80VAA_",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_bow['I'] = df_bow[intro_types_i].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll calculate and add percentage columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['I', 'all']:\n",
    "    df_bow[col+'_perc'] = df_bow[col] / df_bow[col].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tESewJzZVABA",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for col in ['I', 'all']:\n",
    "    df_bow[col+'_perc'] = df_bow[col] / df_bow[col].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print off the dataframe to view what we've done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INFJ</th>\n",
       "      <th>ENTP</th>\n",
       "      <th>INTP</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>ENTJ</th>\n",
       "      <th>ENFJ</th>\n",
       "      <th>INFP</th>\n",
       "      <th>ENFP</th>\n",
       "      <th>ISFP</th>\n",
       "      <th>ISTP</th>\n",
       "      <th>ISFJ</th>\n",
       "      <th>ISTJ</th>\n",
       "      <th>ESTP</th>\n",
       "      <th>ESFP</th>\n",
       "      <th>ESTJ</th>\n",
       "      <th>ESFJ</th>\n",
       "      <th>all</th>\n",
       "      <th>I</th>\n",
       "      <th>I_perc</th>\n",
       "      <th>all_perc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>67821.0</td>\n",
       "      <td>27381.0</td>\n",
       "      <td>52046.0</td>\n",
       "      <td>43810.0</td>\n",
       "      <td>8875.0</td>\n",
       "      <td>8683.0</td>\n",
       "      <td>87642.0</td>\n",
       "      <td>31156.0</td>\n",
       "      <td>11148.0</td>\n",
       "      <td>13883.0</td>\n",
       "      <td>8037.0</td>\n",
       "      <td>8169.0</td>\n",
       "      <td>3704.0</td>\n",
       "      <td>1696.0</td>\n",
       "      <td>1856.0</td>\n",
       "      <td>2166.0</td>\n",
       "      <td>378073</td>\n",
       "      <td>292556.0</td>\n",
       "      <td>0.047701</td>\n",
       "      <td>0.047328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>39658.0</td>\n",
       "      <td>18993.0</td>\n",
       "      <td>35864.0</td>\n",
       "      <td>30497.0</td>\n",
       "      <td>6132.0</td>\n",
       "      <td>5018.0</td>\n",
       "      <td>48004.0</td>\n",
       "      <td>16454.0</td>\n",
       "      <td>6131.0</td>\n",
       "      <td>8893.0</td>\n",
       "      <td>4111.0</td>\n",
       "      <td>5141.0</td>\n",
       "      <td>2191.0</td>\n",
       "      <td>937.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>230224</td>\n",
       "      <td>178299.0</td>\n",
       "      <td>0.029071</td>\n",
       "      <td>0.028820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>40231.0</td>\n",
       "      <td>17852.0</td>\n",
       "      <td>33005.0</td>\n",
       "      <td>28753.0</td>\n",
       "      <td>5889.0</td>\n",
       "      <td>5471.0</td>\n",
       "      <td>48996.0</td>\n",
       "      <td>16945.0</td>\n",
       "      <td>6264.0</td>\n",
       "      <td>8725.0</td>\n",
       "      <td>4607.0</td>\n",
       "      <td>5106.0</td>\n",
       "      <td>2254.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>1223.0</td>\n",
       "      <td>227371</td>\n",
       "      <td>175687.0</td>\n",
       "      <td>0.028646</td>\n",
       "      <td>0.028463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>31931.0</td>\n",
       "      <td>14728.0</td>\n",
       "      <td>26692.0</td>\n",
       "      <td>22778.0</td>\n",
       "      <td>4748.0</td>\n",
       "      <td>3966.0</td>\n",
       "      <td>40375.0</td>\n",
       "      <td>13846.0</td>\n",
       "      <td>4825.0</td>\n",
       "      <td>7124.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>4033.0</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>796.0</td>\n",
       "      <td>841.0</td>\n",
       "      <td>986.0</td>\n",
       "      <td>182870</td>\n",
       "      <td>141091.0</td>\n",
       "      <td>0.023005</td>\n",
       "      <td>0.022892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>31628.0</td>\n",
       "      <td>14236.0</td>\n",
       "      <td>24880.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>4564.0</td>\n",
       "      <td>4343.0</td>\n",
       "      <td>40709.0</td>\n",
       "      <td>15002.0</td>\n",
       "      <td>5153.0</td>\n",
       "      <td>6540.0</td>\n",
       "      <td>3571.0</td>\n",
       "      <td>3827.0</td>\n",
       "      <td>1905.0</td>\n",
       "      <td>834.0</td>\n",
       "      <td>943.0</td>\n",
       "      <td>988.0</td>\n",
       "      <td>180691</td>\n",
       "      <td>137876.0</td>\n",
       "      <td>0.022481</td>\n",
       "      <td>0.022619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         INFJ     ENTP     INTP     INTJ    ENTJ    ENFJ     INFP     ENFP  \\\n",
       "Word                                                                         \n",
       "i     67821.0  27381.0  52046.0  43810.0  8875.0  8683.0  87642.0  31156.0   \n",
       "the   39658.0  18993.0  35864.0  30497.0  6132.0  5018.0  48004.0  16454.0   \n",
       "to    40231.0  17852.0  33005.0  28753.0  5889.0  5471.0  48996.0  16945.0   \n",
       "a     31931.0  14728.0  26692.0  22778.0  4748.0  3966.0  40375.0  13846.0   \n",
       "and   31628.0  14236.0  24880.0  21568.0  4564.0  4343.0  40709.0  15002.0   \n",
       "\n",
       "         ISFP     ISTP    ISFJ    ISTJ    ESTP    ESFP    ESTJ    ESFJ  \\\n",
       "Word                                                                     \n",
       "i     11148.0  13883.0  8037.0  8169.0  3704.0  1696.0  1856.0  2166.0   \n",
       "the    6131.0   8893.0  4111.0  5141.0  2191.0   937.0  1000.0  1200.0   \n",
       "to     6264.0   8725.0  4607.0  5106.0  2254.0   972.0  1078.0  1223.0   \n",
       "a      4825.0   7124.0  3333.0  4033.0  1868.0   796.0   841.0   986.0   \n",
       "and    5153.0   6540.0  3571.0  3827.0  1905.0   834.0   943.0   988.0   \n",
       "\n",
       "         all         I    I_perc  all_perc  \n",
       "Word                                        \n",
       "i     378073  292556.0  0.047701  0.047328  \n",
       "the   230224  178299.0  0.029071  0.028820  \n",
       "to    227371  175687.0  0.028646  0.028463  \n",
       "a     182870  141091.0  0.023005  0.022892  \n",
       "and   180691  137876.0  0.022481  0.022619  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow.sort_values(by='all', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vguk_YpZVABC",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# calculate chi2\n",
    "df_bow['chi2_i'] = np.power((df_bow['I_perc'] - df_bow['all_perc']), 2) / df_bow['all_perc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOesPcaXVABD",
    "outputId": "b973bca5-2d74-4cdd-9a71-fc9956c0914f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I_perc</th>\n",
       "      <th>all_perc</th>\n",
       "      <th>chi2_i</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>urlweb</th>\n",
       "      <td>0.002817</td>\n",
       "      <td>0.002615</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infp</th>\n",
       "      <td>0.001249</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infj</th>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infps</th>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infjs</th>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intp</th>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>0.012057</td>\n",
       "      <td>0.011859</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intps</th>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.047701</td>\n",
       "      <td>0.047328</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0.012361</td>\n",
       "      <td>0.012176</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          I_perc  all_perc    chi2_i\n",
       "Word                                \n",
       "urlweb  0.002817  0.002615  0.000016\n",
       "infp    0.001249  0.001118  0.000015\n",
       "infj    0.001111  0.001019  0.000008\n",
       "infps   0.000466  0.000413  0.000007\n",
       "infjs   0.000393  0.000347  0.000006\n",
       "intp    0.000936  0.000871  0.000005\n",
       "my      0.012057  0.011859  0.000003\n",
       "intps   0.000346  0.000315  0.000003\n",
       "i       0.047701  0.047328  0.000003\n",
       "in      0.012361  0.012176  0.000003"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow[['I_perc', 'all_perc', 'chi2_i']][df_bow['I_perc'] > df_bow['all_perc']].sort_values(by='chi2_i', ascending=False).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dySRM0KqVABE"
   },
   "source": [
    "And there it is! What can we conclude from this?\n",
    "\n",
    "Looking at the top words with higher chi-square values, we can see that words like \"urlweb,\" \"infp,\" \"infj,\" as well as \"i\" have the top chi-square values compared to others. This indicates that these words are used more frequently by introverts than would be expected based on their overall occurrence in the dataset.\n",
    "\n",
    "The word \"I\" appears 9th in the top 10 highest chi-square values of 0.000003, suggesting that its usage by introverts deviates significantly from what would be expected based on its general frequency.\n",
    "\n",
    "Therefore, based on these findings, we can conclude that introverts tend to use \"I\" more frequently than extroverts, supporting the hypothesis that introverts favour the use of the word \"I.\"\n",
    "\n",
    "Let's now have a look at the words most used by extroverts following the same process but for extovert types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E_perc</th>\n",
       "      <th>all_perc</th>\n",
       "      <th>chi2_e</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>enfp</th>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.001124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entp</th>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.001124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entps</th>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enfps</th>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entj</th>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.000397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enfj</th>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estp</th>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.000181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entjs</th>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.000127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enfjs</th>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ne</th>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0.016918</td>\n",
       "      <td>0.015607</td>\n",
       "      <td>0.000110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7w6</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7w8</th>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>0.003015</td>\n",
       "      <td>0.002530</td>\n",
       "      <td>0.000093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         E_perc  all_perc    chi2_e\n",
       "Word                               \n",
       "enfp   0.001632  0.000728  0.001124\n",
       "entp   0.001474  0.000631  0.001124\n",
       "entps  0.000599  0.000226  0.000619\n",
       "enfps  0.000555  0.000228  0.000468\n",
       "entj   0.000738  0.000360  0.000397\n",
       "enfj   0.000631  0.000356  0.000213\n",
       "estp   0.000517  0.000289  0.000181\n",
       "entjs  0.000241  0.000105  0.000176\n",
       "d      0.000657  0.000424  0.000127\n",
       "enfjs  0.000220  0.000106  0.000122\n",
       "ne     0.000504  0.000312  0.000118\n",
       "you    0.016918  0.015607  0.000110\n",
       "7w6    0.000102  0.000038  0.000107\n",
       "7w8    0.000078  0.000027  0.000098\n",
       "he     0.003015  0.002530  0.000093"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract extrovert types only from all the personality types\n",
    "intro_types_e = [p for p in type_labels if p[0] == 'E']\n",
    "#Create an extrovert total word count column, which sums the counts of all extrovert columns\n",
    "df_bow['E'] = df_bow[intro_types_e].sum(axis=1)\n",
    "#calculate and add percentage column for extroverts\n",
    "df_bow['E_perc'] = df_bow['E'] / df_bow['E'].sum()\n",
    "# calculate chi2 for extroverts\n",
    "df_bow['chi2_e'] = np.power((df_bow['E_perc'] - df_bow['all_perc']), 2) / df_bow['all_perc']\n",
    "df_bow[['E_perc', 'all_perc', 'chi2_e']][df_bow['E_perc'] > df_bow['all_perc']].sort_values(by='chi2_e', ascending=False).head(15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the chi-squared analysis, there is evidence to suggest that extroverts tend to use words like \"enfp\", \"entp\", \"entps\", and \"enfps\" as well as \"you\" more frequently compared to their overall usage. This supports our hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TlO1q-zlVABg"
   },
   "source": [
    "### n-grams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UXZa-xFeVABh"
   },
   "source": [
    "While individual words do carry meaning, it is often the case that combinations of words change meanings of sentences entirely.  For example, what difference does removing the `not` from a sentence make?\n",
    "\n",
    "Natural Language Processing is **not** easy!\n",
    "\n",
    "n-grams are a method to extract combinations of words into features for model building.  The `n` in n-grams specifies the number of tokens to include.  For example, a 2-gram returns all the consecutive pairs of words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nGJEya0iVABi",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-W1NOt4VABi",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def word_grams(words, min_n=1, max_n=4):\n",
    "    s = []\n",
    "    for n in range(min_n, max_n):\n",
    "        for ngram in ngrams(words, n):\n",
    "            s.append(' '.join(str(i) for i in ngram))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0uyHBnmoVABj",
    "outputId": "c5408f67-e2e2-40a6-a7c4-e31c5c359853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'two', 'three', 'four', 'one two', 'two three', 'three four', 'one two three', 'two three four']\n"
     ]
    }
   ],
   "source": [
    "print (word_grams('one two three four'.split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine consecutive words into groups of 2 using n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7jR-KSHUVABm",
    "outputId": "b2a4e948-4a7b-4c86-9a98-b2f530bd3174"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'find'),\n",
       " ('find', 'all'),\n",
       " ('all', 'of'),\n",
       " ('of', 'you'),\n",
       " ('you', 'to'),\n",
       " ('to', 'be'),\n",
       " ('be', 'extremely'),\n",
       " ('extremely', 'humorous'),\n",
       " ('humorous', 'now'),\n",
       " ('now', 'to'),\n",
       " ('to', 'find'),\n",
       " ('find', 'other'),\n",
       " ('other', 'specimen'),\n",
       " ('specimen', 'to'),\n",
       " ('to', 'observe')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in ngrams(all_mbti.iloc[55555]['tokens'], 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine consecutive words into groups of 3 using n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BVUBu-36VABo",
    "outputId": "882045f8-7e10-4f47-d939-bf127459d543"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'find', 'all'),\n",
       " ('find', 'all', 'of'),\n",
       " ('all', 'of', 'you'),\n",
       " ('of', 'you', 'to'),\n",
       " ('you', 'to', 'be'),\n",
       " ('to', 'be', 'extremely'),\n",
       " ('be', 'extremely', 'humorous'),\n",
       " ('extremely', 'humorous', 'now'),\n",
       " ('humorous', 'now', 'to'),\n",
       " ('now', 'to', 'find'),\n",
       " ('to', 'find', 'other'),\n",
       " ('find', 'other', 'specimen'),\n",
       " ('other', 'specimen', 'to'),\n",
       " ('specimen', 'to', 'observe')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in ngrams(all_mbti.iloc[55555]['tokens'], 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2sm4dVWzVABG"
   },
   "source": [
    "## Now that we understand all of that, let's cheat!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aUvfk_qrVABG"
   },
   "source": [
    "**Praise be to Python...**\n",
    "\n",
    "`sklearn` has a built in text feature extraction module called [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) that will literally do all of that work in one line of code! This function will convert a collection of documents (rows of text) into a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RTbEo8uEVABH",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fe57kvsHVABI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "# Fit the CountVectorizer on the preprocessed 'post' column\n",
    "vect.fit(all_mbti['post'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oFzCFS89VABM"
   },
   "source": [
    "### Tuning the vectorizer\n",
    "\n",
    "We have been using the default parameters of [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). However, the vectorizer is worth tuning, just like a model is worth tuning! Here are a few parameters that you might want to tune with examples on how to do so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TqeL2KJ4VABR"
   },
   "source": [
    "- **stop_words:** string 'english', list, or None (default)\n",
    "    * If 'english', a built-in stop word list for English is used.\n",
    "    * If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "    * If None, no stop words will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzEOoIt0VABR",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# remove English stop words\n",
    "vect = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WglDIhfhVABU"
   },
   "source": [
    "- **ngram_range:** tuple (min_n, max_n), default=(1, 1)\n",
    "    - The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n",
    "    - All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2wx1TyjjVABU",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nfz8TkhbVABW"
   },
   "source": [
    "- **max_df:** float in range [0.0, 1.0] or int, default=1.0\n",
    "    - When building the vocabulary, ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n",
    "    - If float, the parameter represents a proportion of documents.\n",
    "    - If integer, the parameter represents an absolute count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z5B_nbBDVABW",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ignore terms that appear in more than 50% of the documents\n",
    "vect = CountVectorizer(max_df=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7CMv962vVABY"
   },
   "source": [
    "- **min_df:** float in range [0.0, 1.0] or int, default=1\n",
    "    - When building the vocabulary, ignore terms that have a document frequency strictly lower than the given threshold. (This value is also called \"cut-off\" in the literature.)\n",
    "    - If float, the parameter represents a proportion of documents.\n",
    "    - If integer, the parameter represents an absolute count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M8OcjOK9VABZ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# only keep terms that appear in at least 2 documents\n",
    "vect = CountVectorizer(min_df=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3GTogGUiVABa"
   },
   "source": [
    "### Guidelines for tuning CountVectorizer:\n",
    "\n",
    "- Use your knowledge of the **problem** and the **text**, and your understanding of the **tuning parameters**, to help you decide what parameters to tune and how to tune them.\n",
    "- **Experiment**, and let the data tell you the best approach!\n",
    "\n",
    "Finally, let's fit a tuned CountVectorizer to the MBTI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BTPnl452VABa",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "betterVect = CountVectorizer(stop_words='english', \n",
    "                             min_df=2, \n",
    "                             max_df=0.5, \n",
    "                             ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KsB226qGVABc",
    "outputId": "d68d31e5-b3d4-4a3f-bb91-97724bf9521e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_df=0.5, min_df=2, stop_words='english')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betterVect.fit(all_mbti['post'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After vectorization using `CountVectorizer`, we can view the transformed data as a matrix where each row represents a document (post in our case) and each column represents a unique word in the vocabulary. The cell values indicate the count of the corresponding word in each document.\n",
    "\n",
    "It's essential to note that this process generates a very large dataset, potentially consuming significant memory on your machine.\n",
    "\n",
    "Uncomment the code below if you would still want to view the vectorized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Transform the training data\\nvectorized_data = betterVect.transform(all_mbti['post'][0:10000,])\\n\\n# Convert the sparse matrix to a dense array for easier viewing (optional)\\ndense_vectorized_data = vectorized_data.toarray()\\n\\n# Create a DataFrame to display the vectorized data\\nvectorized_df = pd.DataFrame(dense_vectorized_data, columns=betterVect.get_feature_names_out())\\n\\n# Display the vectorized DataFrame\\nprint(vectorized_df)\\n\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Transform the training data\n",
    "vectorized_data = betterVect.transform(all_mbti['post'][0:10000,])\n",
    "\n",
    "# Convert the sparse matrix to a dense array for easier viewing (optional)\n",
    "dense_vectorized_data = vectorized_data.toarray()\n",
    "\n",
    "# Create a DataFrame to display the vectorized data\n",
    "vectorized_df = pd.DataFrame(dense_vectorized_data, columns=betterVect.get_feature_names_out())\n",
    "\n",
    "# Display the vectorized DataFrame\n",
    "print(vectorized_df)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this train we covered various techniques for cleaning text data and extracting features to use with machine learning models. We also demonstrated how NLTK's `CountVectorizer` can be used to clean text data and extract features, transforming the text data into a matrix of numbers that can be fed into a machine learning model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "VCXae5QXU__Z",
    "wzM8TbWBU__h",
    "FvA-QZmRU__r",
    "hAUkklVXU__6",
    "rFln-NFtVAAI",
    "UZomXVzoVAAR",
    "qp-n688CVAAc",
    "tGmGzrbsVAAf",
    "oFzCFS89VABM",
    "TlO1q-zlVABg"
   ],
   "name": "3_How-do-machines-understand language.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
